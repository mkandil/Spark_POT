{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Spark SQL\n",
    "## This Lab will show you how to work with Spark SQL\n",
    "\n",
    "\n",
    "### Spark SQL\n",
    "This notebook guides you through querying data with Apache Spark, including how to create and use DataFrames, run SQL queries, apply functions to the results of SQL queries, join data from different data sources, and visualize data in graphs.\n",
    "\n",
    "This notebook uses pySpark, the Python API for Spark. Some knowledge of Python is recommended. This notebook runs on Python 2 with Spark 1.6 and 2.0.\n",
    "\n",
    "If you are new to Apache Spark, see the first module in this series: __Introduction to Apache Spark: Basic Concepts__.\n",
    "\n",
    "Before you can run SQL queries on data in an Apache Spark environment, you need to enable SQL processing and then move the data to the structured format of a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Enable SQL processing\n",
    "The way you enable SQL processing with Spark 1.6 is to create an sqlContext. With Spark 2.0, the preferred method is to use the new sparkSession object, but the sqlContext object is still supported. \n",
    "\n",
    "Use the predefined Spark Context, `sc`, which contains the connection information for Spark, to create a sqlContext:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "\n",
    "<h3>Getting started: Create a SQL Context</h3>\n",
    "\n",
    "<b>Type:</b>\n",
    "\n",
    "\n",
    "from pyspark.sql import SQLContext<br>\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the SQLContext below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "\n",
    "<h3>Dowload a JSON Recordset to work with</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data, we can run commands on the console of the server (or docker image) that the notebook enviroment is using. To do so we simply put a \"!\" in front of the command that we want to run. For example:\n",
    "\n",
    "!pwd\n",
    "\n",
    "To get the data we will download a file to the enviroment. Simple run these two commands, the first just ensures that the file is removed if it exists:\n",
    "\n",
    "!rm world_bank.json.gz -f <br>\n",
    "!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#enter the commands to remove and download file here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "<h3>Create a Dataframe</h3>\n",
    "\n",
    "Now you can create the Dataframe, note that if you wanted to see where you downloaded the file you can run !pwd or !ls\n",
    "\n",
    "Instead of creating an RDD to read the file, you'll create a Spark DataFrame. Unlike an RDD, a DataFrame creates a schema around the data, which supplies the necessary structure for SQL queries. A self-describing format like JSON is ideal for DataFrames, but many other file types are supported, including text (CSV) and Parquet.\n",
    "\n",
    "To create the Dataframe type:\n",
    "\n",
    "example1_df = sqlContext.read.json(\"./world_bank.json.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the Dataframe here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>We can look at the schema:</h3>\n",
    "\n",
    "Type or copy/paste the following into the cell below:<br>\n",
    "example1_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print out the schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataframes work like RDDs, you can map, reduce, groupby, etc. \n",
    "<br>Take a look at the first two rows of data using \"take(2)\". <br></h3>\n",
    "Type of copy/paste the following into the cell below:<br>\n",
    "example1_df.take(2)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use take on the dataframe to pull out 2 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For increased readability, run the cell below to include a row of asterisks in between the data rows:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in example1_df.take(2):\n",
    "    print row\n",
    "    print \"*\" * 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 \n",
    "<h3>Register a table</h3>\n",
    "\n",
    "Using\n",
    "DataframeName.registerTempTable(\"name_of_table\")\n",
    "\n",
    "Create a table named \"world_bank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create the table to be referenced via SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5\n",
    "<h3>Running SQL Statements</h3>\n",
    "Using SQL, get the first 2 records from the table defined in the cell above (This is done using the SQL \"limit 2\" clause).<br>\n",
    "#### Hint: sqlContext.sql(\"SQL Statement\") will execute the SQL statement between double quotes and return a Dataframe with the records corresponding to the result of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use SQL to select from table limit 2 and print the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The method show(n) shows the first n elements of a dataframe. Use it below to display the rows resulting from your SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the content of the dataframe resulting from your SQL query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that the data produced by show() is not very nicely formatted... We can take care of this by converting the Spark dataframe to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extra credit, take the Dataframe you created with the two records and convert it into Pandas. This is done by applying the .toPandas() method \n",
    "# to the SparkSQL dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Warning: Converting a Spark dataframe to Pandas effectively executes a \"collect\" on the Spark dataframe. What is a potential problem with this?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also run the following cell to select all columns from the table and print information about the resulting DataFrame and schema of the data: (assuming you named your table world_bank). Notice that printing a dataframe does not print its \"contents\". Remember that you need to use .show() to display the contents of a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_df =  sqlContext.sql(\"select * from world_bank\")\n",
    "\n",
    "print type(temp_df)\n",
    "print \"*\" * 20\n",
    "print temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Review of Spark concepts</span>\n",
    "You should notice that executing:<br><span style=\"color:blue\">sqlContext.sql(\"SQL statement\")</span><span style=\"color:black\"><br> with any SQL statement runs almost immediately, but<br><span style=\"color:blue\">sqlContext.sql(\"SQL statement\").count()</span><span style=\"color:black\"> should take longer to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try the test above in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Can you tell why this is happening?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with an aggregation in SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now using the same \"world_bank\" table, let's produce a simple count based on grouping over a column, for example \"regionname\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With JSON data you can reference the nested data\n",
    "# If you look at Schema above you can see that Sector.Name is a nested column\n",
    "# Select that column and limit to reasonable output (like 2)\n",
    "\n",
    "#sqlContext.sql(\"select id, sector.Name from world_bank limit 2\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6\n",
    "\n",
    "<h3>Creating simple graphs</h3>\n",
    "Using Pandas and the matplotlib library, we can create some simple visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First create a SQL statement that returns a resonable number of elements.\n",
    "For example, we can count the number of projects (rows) by countryname. But before that, let's go through a quick setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to tell the charting library (matplotlib) to display charts inline\n",
    "# just run this cell\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Now we can write simple sql to look at some data. Remember to add .toPandas() for nicer formatting. A slightly different option than what we've used so far is to create a variable and set it to the SQL statement. The cell below gives you some help with commented code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query = \"select count(*) as Count, countryname from world_bank group by countryname\"\n",
    "# chart1_df = sqlContext.sql(query).toPandas()\n",
    "# print chart1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now using the matplotlib .plot method, we can then plot data from the previous Pandas dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now take the variable (or same sql statement) and use the method:\n",
    "# .plot(kind='bar', x='countryname', y='Count', figsize=(12, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">The chart produced above is very \"busy\" on the x-axis. Can you produce a less busy chart?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7\n",
    "\n",
    "<h3>Creating a dataframe \"manually\" by adding a schema to an RDD</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we need to create an RDD of pairs or triplets. This can be done using some code (such as a for loop) as seen in the instructor's example, or more simply by assigning values to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Default array defined below. Feel free to change as desired.\n",
    "array=[[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5]]\n",
    "my_rdd = sc.parallelize(array)\n",
    "my_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below this one, create your dataframe manually, following these steps:<br>\n",
    "1- Define your schema columns as a string<br>\n",
    "2- Build the schema object using StructField<br>\n",
    "3- Apply the schema object to the RDD<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The cell below is missing some code and will not run properly until the missing code has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# The schema is encoded in a string. Complete the string below\n",
    "schemaString = \"\"\n",
    "\n",
    "# MissingType() should be either StringType() or IntegerType(). Please replace as required.\n",
    "fields = [StructField(field_name, MissingType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaExample = sqlContext.createDataFrame(use_your_rdd_name_here, schema)\n",
    "\n",
    "# Register the DataFrame as a table. Add table name below as parameter to registerTempTable.\n",
    "schemaExample.registerTempTable(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run some select statements on your newly created DataFrame and display the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from a dashDB datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a different browser tab, create a dashDB service, add credentials and come back to this notebook. <br>Each dashDB instance in bluemix is created with a \"GOSALES\" set of tables which we can reuse for the purpose of this example. (You can create your own table if you wish...)<br><br>Replace the Xs in the cell below with proper credentials and verify access to dashDB tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salesDF = sqlContext.read.format('jdbc').\\\n",
    "          options(url='jdbc:db2://dashdb-entry-yp-xxxxx.services.dal.bluemix.net:50000/BLUDB:user=xxxxx;password=xxxxxx;',\\\n",
    "                  dbtable='GOSALES.BRANCH').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salesDF.show(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}