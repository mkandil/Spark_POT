{"nbformat_minor": 0, "cells": [{"source": "## Lab 1 - Hello Spark\nThis Lab will show you how to work with Apache Spark using Python", "cell_type": "markdown", "metadata": {}}, {"source": "###Step 1 - Working with Spark Context\n\nStep 1 - Invoke the spark context and extract what version of the spark driver application.\n\nType<br>\nsc.version", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "#Step 1.1 - Check spark version\nsc.version", "outputs": [{"execution_count": 1, "output_type": "execute_result", "data": {"text/plain": "u'1.6.0'"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "###Step 2 - Working with Resilient Distributed Datasets\n\nStep 2 - Create RDD with numbers 1 to 10,<br>\nExtract first line,<br>\nExtract first 5 lines,<br>\nCreate RDD with string \"Hello Spark\",<br>\nExtract first line.<br>\n\nType: <br>\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br>\nx_nbr_rdd = sc.parallelize(x)<br>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "#Step 2.1 - Create RDD of Numbers 1-10\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nx_nbr_rdd = sc.parallelize(x)", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"source": "Type: <br>\nx_nbr_rdd.first()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "#Step 2.2 - Extract first line\nx_nbr_rdd.first()", "outputs": [{"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "1"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Type:<br>\nx_nbr_rdd.take(5)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "#Step 2.3 - Extract first 5 lines\nx_nbr_rdd.take(5)", "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": "[1, 2, 3, 4, 5]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Perform a first map transformation and rpelace each element X in the RDD with X+1.<br>\nType:<br>\nx_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "#Step 2.4 - Perform your first map transformation. Replace each element X in the RDD with X+1.\n#Remember that RDDs are IMMUTABLE, so it is not possible to UPDATE an RDD. You need to create\n#a NEW RDD\nx_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"source": "Take a look at the elements of the new RDD.<br>\nType:<br>\nx_nbr_rdd_2.collect()   ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "#Step 2.5 - Check out the elements of the new RDD. Warning: Be careful with this in real life !! As you\n#will be bringing all elements of the RDD (from all partitions) to the driver...\nx_nbr_rdd_2.collect()", "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Let's now create a new RDD with one string \"Hello Spark\" and take a look at it.<br>\nType:<br>\ny = [\"Hello Spark!\"]<br>\ny_str_rdd = sc.parallelize(y)<br>\ny_str_rdd.first()<br>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "#Step 2.6 - Create String RDD, Extract first line\ny = [\"Hello Spark!\"]\ny_str_rdd = sc.parallelize(y)\ny_str_rdd.first()", "outputs": [{"execution_count": 7, "output_type": "execute_result", "data": {"text/plain": "'Hello Spark!'"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Let's now create a third RDD with several strings.<br>\nType:<br>\nz = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]<br>\nz_str_rdd = sc.parallelize(z)<br>\nz_str_rdd.first()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "#Step 2.7 - Create String RDD with many lines / entries, Extract first line\nz = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]\nz_str_rdd = sc.parallelize(z)\nz_str_rdd.first()", "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "'First,Line'"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Count the number of entries in this RDD.<br>\nType:<br>\nz_str_rdd.count()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "#Step 2.8 - Count the number of entries in the RDD\nz_str_rdd.count()", "outputs": [{"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": "3"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Take a look at the elements of this RDD.<br>\nType:<br>\nz_str_rdd.collect()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "#Step 2.9 - Show all the entries in the RDD. Warning: Be careful with this in real life !! \n#As you will be bringing all elements of the RDD (from all partitions) to the driver...\nz_str_rdd.collect()", "outputs": [{"execution_count": 10, "output_type": "execute_result", "data": {"text/plain": "['First,Line', 'Second,Line', 'and,Third,Line']"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "In the next step, we will split all the entries in the RDD on the commas \",\" <br>\nType: <br>\nz_str_rdd_split = z_str_rdd.map(lambda line: line.split(\",\"))<br>\nz_str_rdd_split.collect()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "#Step 2.10 - Perform a map transformation to split all entries in the RDD on the commas \",\".\nz_str_rdd_split = z_str_rdd.map(lambda line: line.split(\",\"))\n\n#Check out the entries in the new RDD\nz_str_rdd_split.collect()\n\n#Notice how the entries in the new RDD are now ARRAYs with elements, where the original\n#strings have been split using the comma delimiter.", "outputs": [{"execution_count": 11, "output_type": "execute_result", "data": {"text/plain": "[['First', 'Line'], ['Second', 'Line'], ['and', 'Third', 'Line']]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "In this step, we will learn a new transformation besides map: flatMap <br>\nflatMap will \"flatten\" all the elements of an RDD entry into its subcomponents<br>\nThis is better explained with an example<br>\nType:<br>\nz_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\")<br>\nz_str_rdd_split_flatmap.collect()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "#Step 2.11 - Learn the difference between two transformations: map and flatMap.\n#Go back to the RDD z_str_rdd_split defined above using a map transformation from z_str_rdd\n#and use this time a flatmap.\nz_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))\nz_str_rdd_split_flatmap.collect()\n\n#What do you notice ? How is z_str_rdd_split_flatmap different from z_str_rdd_split ?", "outputs": [{"execution_count": 12, "output_type": "execute_result", "data": {"text/plain": "['First', 'Line', 'Second', 'Line', 'and', 'Third', 'Line']"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "In this step, we will augment each entry in the previous RDD with the number \"1\" to create pairs (or tuples). The first element of the tuple will be the keyword and the second elements of the tuple will be the digit \"1\".<br>\nThis is a common technic used to count elements using Spark.<br>\nType:<br>\ncountWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))<br>\ncountWords.collect()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "#Step 2.12 - Learn the difference between two transformations: map and flatMap.\n#Go back to the RDD z_str_rdd_split defined above using a map transformation from z_str_rdd\n#and use this time a flatmap.\n\ncountWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))\ncountWords.collect()", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "[('First', 1),\n ('Line', 1),\n ('Second', 1),\n ('Line', 1),\n ('and', 1),\n ('Third', 1),\n ('Line', 1)]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Now we have above what is known as a PAIR RDD. Each entry in the RDD has a KEY and a VALUE.<br>\nThe KEY is the word (First, Line, etc...) and the value is the number \"1\"<br>\nWe can now AGGREGATE this RDD by summing up all the values BY KEY<br>\nType:<br>\nfrom operator import add<br>\ncountWords2 = countWords.reduceByKey(add)<br>\ncountWords2.collect()<br>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "#Step 2.13 - Check out the results of the aggregation\nfrom operator import add\ncountWords2 = countWords.reduceByKey(add)\ncountWords2.collect()\n\n#You just created an RDD countWords2 which contains the counts for each token...", "outputs": [{"execution_count": 14, "output_type": "execute_result", "data": {"text/plain": "[('and', 1), ('Second', 1), ('Third', 1), ('Line', 3), ('First', 1)]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "###Step 3 - Count number of lines with Spark in it\nStep 3 - Pull in a spark README.md file, <br>\nConvert the file to an RDD,<br>\nCount the number of lines with the word \"Spark\" in it. <br>\n\nType:<br>\n!rm README.md* -f<br>\n!wget https://github.com/carloapp2/SparkPOT/blob/master/README.md<br>\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "#Step 3.1 - Pull data file into workbench\n!rm README.md* -f\n!wget https://github.com/carloapp2/SparkPOT/blob/master/README.md", "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2016-04-19 05:43:31--  https://github.com/carloapp2/SparkPOT/blob/master/README.md\nResolving github.com (github.com)... 192.30.252.121\nConnecting to github.com (github.com)|192.30.252.121|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: \u2018README.md\u2019\n\n    [ <=>                                   ] 40,241      --.-K/s   in 0.04s   \n\n2016-04-19 05:43:31 (935 KB/s) - \u2018README.md\u2019 saved [40241]\n\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Now we will point Spark to the text file stored in the local filesystem and use the \"textFile\" method to create an RDD named \"textfile_rdd\" which will contain one entry for each line in the original text file.<br>\nWe will also count the number of lines in the RDD (which would be as well the number of lines in the text file. <br>\nType:<br>\ntextfile_rdd = sc.textFile(\"/resources/README.md\")<br>\ntextfile_rdd.count()<br>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "#Step 3.2 - Create RDD from data file\ntextfile_rdd = sc.textFile(\"/resources/README.md\")\ntextfile_rdd.count()", "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "595"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Let us now filter out the RDD and only keep the entries that contain the token \"Spark\". This will be achieved using the \"filter\" transformation, combined with the Python syntax for figuring out whether a particular substring is present within a larger string: substring in string.<br>\nWe will also take a look at the first line in the newly filtered RDD. <br>\nType:<br>\nSpark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)<br>\nSpark_lines.first()<br>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "#Step 3.3 - Filter for only lines with word Spark\nSpark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\nSpark_lines.first()", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "u'    <title>SparkPOT/README.md at master \\xb7 carloapp2/SparkPOT \\xb7 GitHub</title>'"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "We will now count the number of entries in this filtered RDD and present the result as a concatenated string.<br>\nType:<br>\nprint \"The file README.md has \" + str(Spark_lines.count()) + \\<br>\n\" of \" + str(textfile_rdd.count()) + \\<br>\n\" Lines with word Spark in it.\"<br>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "#Step 3.4 - count the number of lines\nprint \"The file README.md has \" + str(Spark_lines.count()) + \\\n\" of \" + str(textfile_rdd.count()) + \\\n\" Lines with the word Spark in it.\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The file README.md has 49 of 595 Lines with the word Spark in it.\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Using your knowledge from the previous exercises, you will now count the number of times the substring \"Spark\" appears in the original text.<br>\nInstructions:<br>\nLooking back at previous exercises, you will need to: <br>\n1- Execute a flatMap transformation on the original RDD Spark_lines and split on white space.<br>\n2- Augment each token with the digit \"1\", so as to obtain a PAIR RDD where the first element of the tuple is the token and the second element is the digit \"1\".<br>\n3- Execute a reduceByKey with the addition to count the number of instances of each token.<br>\n4- Filter the resulting RDD from Step 3- above to only keep entries which start with \"Spark\".<br> In Python, the syntax to decide whether a string starts with a token is string.startswith(\"token\"). <br>\n5- Display the resulting list of tokens which start with \"Spark\".", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "#Step 3.5 - count the number of instances of tokens starting with \"Spark\"\ntemp = Spark_lines.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1)).reduceByKey(add)\ntemp.filter(lambda (k,v): k.startswith(\"Spark\")).collect()", "outputs": [{"execution_count": 19, "output_type": "execute_result", "data": {"text/plain": "[(u'SparkPOT:master\"', 1),\n (u'Spark</h2>', 1),\n (u'Spark', 11),\n (u'Spark.</p>', 1),\n (u'SparkPi', 2),\n (u'Spark</a>.', 1),\n (u'Spark\"</a>.</p>', 1),\n (u'Spark</h1>', 1)]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "As a slight modification of the cell above, let us now filter out and display the tokens which contain the substring \"Spark\". (Instead of those which only START with it). Your result should be a superset of the previous result. <br>\nThe Python syntax to determine whether a string contains a particular \"token\" is: \"token\" in string<br>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "#Step 3.6 - Display the tokens which contain the substring \"Spark\" in them.\ntemp.filter(lambda (k,v): \"Spark\" in k).collect()", "outputs": [{"execution_count": 20, "output_type": "execute_result", "data": {"text/plain": "[(u'href=\"/carloapp2/SparkPOT\"', 2),\n (u'href=\"https://github.com/carloapp2/SparkPOT/commits/master.atom\"', 1),\n (u'href=\"/carloapp2/SparkPOT/issues\"', 1),\n (u'SparkPOT:master\"', 1),\n (u'content=\"github.com/carloapp2/SparkPOT', 1),\n (u'content=\"https://github.com/carloapp2/SparkPOT\"', 1),\n (u'href=\"/carloapp2/SparkPOT/pulse\"', 1),\n (u'href=\"/login?return_to=%2Fcarloapp2%2FSparkPOT%2Fblob%2Fmaster%2FREADME.md\"',\n  1),\n (u'Spark</h2>', 1),\n (u'href=\"/carloapp2/SparkPOT/blame/master/README.md\"', 1),\n (u'/carloapp2/SparkPOT/graphs\">', 1),\n (u'href=\"/carloapp2/SparkPOT\"><span>SparkPOT</span></a></span></span><span',\n  1),\n (u'href=\"https://github.com/carloapp2/SparkPOT/blob/master/README.md\"', 1),\n (u'href=\"/carloapp2/SparkPOT/watchers\">', 1),\n (u'Spark', 11),\n (u'Spark.</p>', 1),\n (u'/carloapp2/SparkPOT\"', 1),\n (u'/carloapp2/SparkPOT/issues\"', 1),\n (u'https://github.com/carloapp2/SparkPOT.git\">', 1),\n (u'href=\"/carloapp2/SparkPOT/network\"', 1),\n (u'carloapp2/SparkPOT', 1),\n (u'SparkPi', 2),\n (u'href=\"/carloapp2/SparkPOT/graphs\"', 1),\n (u'href=\"https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting\">run',\n  1),\n (u'href=\"/carloapp2/SparkPOT/commits/master/README.md\"', 1),\n (u'Spark</a>.', 1),\n (u'data-scoped-search-url=\"/carloapp2/SparkPOT/search\"', 1),\n (u'href=\"/login?return_to=%2Fcarloapp2%2FSparkPOT\"', 3),\n (u'<title>SparkPOT/README.md', 1),\n (u'action=\"/carloapp2/SparkPOT/search\"', 1),\n (u'href=\"/carloapp2/SparkPOT/blob/bba389f96af3e0a860cc81de59500c25156f95f3/README.md\"',\n  1),\n (u'href=\"/carloapp2/SparkPOT/blob/master/README.md\"', 1),\n (u'src=\"/carloapp2/SparkPOT/contributors/master/README.md\">', 1),\n (u'href=\"/carloapp2/SparkPOT/stargazers\">', 1),\n (u'content=\"SparkPOT', 3),\n (u'<p>Spark', 4),\n (u'/carloapp2/SparkPOT/pulls\"', 1),\n (u'/carloapp2/SparkPOT/pulse\">', 1),\n (u'Spark\"</a>.</p>', 1),\n (u'href=\"/carloapp2/SparkPOT/find/master\"', 1),\n (u'content=\"carloapp2/SparkPOT\"', 4),\n (u'href=\"/carloapp2/SparkPOT/pulls\"', 1),\n (u'data-pjax=\"#js-repo-pjax-container\">SparkPOT</a></strong>', 1),\n (u'Spark</h1>', 1),\n (u'href=\"/carloapp2/SparkPOT/raw/master/README.md\"', 1)]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "###Step 4 - Perform analysis on a data file\nWe have a sample file with instructors and scores. In this exercise we want you to add all scores and report on results by following these steps:<br>\n\n1- The name of the file is \"Scores.txt\". Delete it from the local filesystem if it exists.<br>\n2- Download the file from the provided location (see below).<br>\n3- Load the text file into an RDD of instructor names and instructor scores.<br>\n4- Execute a transformation which will keep the instructors names, but will add up the 4 numbers representing the scores per instructor, resulting into a new RDD<br>\n5- Display the instructor's name and the total score for each instructor<br>\n6- Execute a second transformation to compute the average score for each instructor and display the results.<br>\n7- Who was top performer?<br>\n\nThe Data File has the following format: Instructor Name,Score1,Score2,Score3,Score4<br>\nHere is an example line from the text file: \"Carlo,5,3,3,4\"<br>\nData File Location: https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "#Step 4.1 - Delete the file if it exists, download a new copy and load it into an RDD\n!rm Scores.txt* -f\n!wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt\n\nRaw_Rdd = sc.textFile(\"/resources/Scores.txt\")\nRaw_Rdd.take(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2016-04-19 06:07:07--  https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 23.235.39.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|23.235.39.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75 [text/plain]\nSaving to: \u2018Scores.txt\u2019\n\n100%[======================================>] 75          --.-K/s   in 0s      \n\n2016-04-19 06:07:08 (6.62 MB/s) - \u2018Scores.txt\u2019 saved [75/75]\n\n"}, {"execution_count": 19, "output_type": "execute_result", "data": {"text/plain": "[u'Carlo,5,3,3,4',\n u'Mokhtar,2,5,5,3',\n u'Jacques,4,2,4,5',\n u'Braden,5,3,2,5',\n u'Chris,5,4,5,1']"}, "metadata": {}}], "metadata": {"scrolled": true, "collapsed": false, "trusted": false}}, {"execution_count": 20, "cell_type": "code", "source": "#Step 4.2 - Execute the necessary transformation(s) to extract the instructor's name, as well\n# as the instructors scores, then add up the scores per instructor and display the results\n# in the form of a new RDD with the elements: \"Instructor Name\", InstructorTotals\nSumScores = Raw_Rdd.map(lambda l: l.split(\",\")).\\\nmap(lambda v : (v[0], int(v[1])+int(v[2])+int(v[3])+int(v[4])))\nSumScores.take(5)", "outputs": [{"execution_count": 20, "output_type": "execute_result", "data": {"text/plain": "[(u'Carlo', 15),\n (u'Mokhtar', 15),\n (u'Jacques', 15),\n (u'Braden', 15),\n (u'Chris', 15)]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 21, "cell_type": "code", "source": "#Step 4.3 - Execute additional transformation(s) to compute the average score per instructor.\n# Display the resulting averages for all instructors.\nFinal = SumScores.map(lambda avg: (avg[0],avg[1],avg[1]/4))\nFinal.take(5)", "outputs": [{"execution_count": 21, "output_type": "execute_result", "data": {"text/plain": "[(u'Carlo', 15, 3),\n (u'Mokhtar', 15, 3),\n (u'Jacques', 15, 3),\n (u'Braden', 15, 3),\n (u'Chris', 15, 3)]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}