{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding w/ Apache Spark: Basic Concepts\n",
    "This notebook guides you through the basic concepts to start working with Apache Spark, including how to set up your environment, create and analyze data sets, and work with data files.\n",
    "\n",
    "This notebook uses pySpark, the Python API for Spark. Some knowledge of Python is recommended. This notebook runs in either Spark 1.6 or 2.0.\n",
    "\n",
    "<span style=\"color:blue\">If you are new to notebooks, here's how the user interface works:</span> [Parts of a notebook](http://datascience.ibm.com/docs/content/analyze-data/parts-of-a-notebook.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Apache Spark\n",
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for processing structured data, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/spark.png?raw=true' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "\n",
    "A Spark application has a driver program and executors. Executors run on cluster nodes or in local threads. Data sets are distributed\u001d",
    " across executors. \n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/Spark%20Architecture.png?raw=true' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "In the first four sections of this notebook, you'll learn about Spark with very simple examples. In the last two sections, you'll use what you learned to analyze data files that have more realistic data sets.\n",
    "\n",
    "1. [Work with the SparkContext](#sparkcontext)<br>\n",
    "    1.1 [Invoke the SparkContext](#sparkcontext1)<br>\n",
    "    1.2 [Check the Spark version](#sparkcontext2)<br>\n",
    "2. [Work with RDDs](#rdd)<br>\n",
    "    2.1 [Create a collection](#rdd1)<br>\n",
    "    2.2 [Create an RDD](#rdd2)<br>\n",
    "    2.3 [View the data](#rdd3)<br>\n",
    "    2.4 [Create another RDD](#rdd4)<br>\n",
    "3. [Manipulate data in RDDs](#trans)<br>\n",
    "    3.1 [Update numeric values](#trans1)<br>\n",
    "    3.2 [Split and count strings](#trans2)<br>\n",
    "    3.3 [Counts words with a pair RDD](#trans3)<br>\n",
    "4. [Filter data](#filter)<br>\n",
    "5. [Analyze text data from a file](#wordfile)<br>\n",
    "    5.1 [Get the data from a URL](#wordfile1)<br>\n",
    "    5.2 [Create an RDD from the file](#wordfile2)<br>\n",
    "    5.3 [Filter for a word](#wordfile3)<br>\n",
    "    5.4 [Count instances of a string at the beginning of words](#wordfile4)<br>\n",
    "    5.5 [Count instances of a string within words](#wordfile5)<br>\n",
    "6. [Analyze numeric data from a file](#numfile)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkcontext\"></a>\n",
    "## 1. Work with the SparkContext object\n",
    "\n",
    "The Apache Spark driver application uses the SparkContext object to allow a programming interface to interact with the driver application. The SparkContext can be thought of as the equivalent of a database connection.\n",
    "\n",
    "The Data Science Experience notebook environment predefines the spark context for you as a variable named \"sc\".\n",
    "\n",
    "In other environments, you need to pick an interpreter (for example, pyspark for Python) and create a SparkConf object to initialize a SparkContext object. For example:\n",
    "<br>\n",
    "`from pyspark import SparkContext, SparkConf`<br>\n",
    "`conf = SparkConf().setAppName(appName).setMaster(master)`<br>\n",
    "`sc = SparkContext(conf=conf)`<br>\n",
    "\n",
    "<a id=\"sparkcontext1\"></a>\n",
    "### 1.1 Invoke the SparkContext\n",
    "Run the following cell to see the spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkcontext2\"></a>\n",
    "### 1.2 Check the Spark version\n",
    "Check the version of the Spark driver application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Science Experience also supports other versions of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd\"></a>\n",
    "## 2. Work with Resilient Distributed Datasets\n",
    "Apache Spark uses an abstraction for working with data called a Resilient Distributed Dataset (RDD). An RDD is a collection of elements that can be operated on in parallel. RDDs are immutable, so you can't update the data in them. To update data in an RDD, you must create a new RDD. In Apache Spark, all work is done by creating new RDDs, transforming existing RDDs, or using RDDs to compute results. When working with RDDs, the Spark driver application automatically distributes the work across the cluster.\n",
    "\n",
    "You can construct RDDs by parallelizing existing Python collections (lists), by manipulating RDDs, or by manipulating files in HDFS or any other storage system.\n",
    "\n",
    "You can run these types of methods on RDDs: \n",
    " - Actions: Process the data and return a result\n",
    " - Transformations: Process data and return new RDDs. \n",
    "\n",
    "Find more information on Python methods in the [PySpark documentation](http://spark.apache.org/docs/latest/api/python/pyspark.html).\n",
    "\n",
    "<a id=\"rdd1\"></a>\n",
    "### 2.1 Create a collection\n",
    "Create a Python collection of the numbers 1 - 10:<br><br>\n",
    "type or copy and paste:<br> \n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd2\"></a>\n",
    "### 2.2 Create an RDD \n",
    "Put the collection into an RDD named `x_nbr_rdd` using the `parallelize` method:<br><br>\n",
    "type or copy and paste:<br>\n",
    "x_nbr_rdd = sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there's no return value. The `parallelize` method didn't compute a result, because it is a transformation. At this point, Spark recorded how to create the RDD but did not process any data yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd3\"></a>\n",
    "### 2.3 View the data \n",
    "View the first element in the RDD:<br><br>\n",
    "type or copy and paste:<br>\n",
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in the original collection (x) has been mapped into an \"entry\" or \"element\" in the RDD x_nbr_rdd. Since `first()` is an action, it will trigger the actual processing of the data including the parallelization of the Python list into the RDD and returning the first value. \n",
    "\n",
    "Now view the first five elements in the RDD:<br><br>\n",
    "type or copy and paste:<br>\n",
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd4\"></a>\n",
    "### 2.4 Create another RDD \n",
    "Create a Python collection that contains strings:<br><br>\n",
    "type or copy and paste:<br>\n",
    "y = [\"Hello Human\", \"My Name is Spark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize the collection into an RDD:<br><br>\n",
    "type or copy and paste:<br>\n",
    "y_str_rdd = sc.parallelize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the first element in the RDD:<br><br>\n",
    "type or copy and paste:<br>\n",
    "y_str_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans\"></a>\n",
    "## 3. Manipulate data in RDDs\n",
    "\n",
    "Remember that to manipulate data, you use transformations.\n",
    "\n",
    "Here are some common transformations that you'll be using in this notebook:\n",
    "\n",
    " - `map(func)`: returns a new RDD with the results of running the specified function on each element  \n",
    " - `filter(func)`: returns a new RDD with the elements for which the specified function returns true   \n",
    " - `distinct([numTasks]))`: returns a new RDD that contains the distinct elements of the source RDD\n",
    " - `flatMap(func)`: returns a new RDD by first running the specified function on all elements, returning 0 or more results for each original element, and then flattening the results into individual elements (this will be clarified with an example)\n",
    "\n",
    "You can also create functions that run a single expression and don't have a name with the Python `lambda` keyword. For example, this function returns the sum of its arguments: `lambda a , b : a + b`.\n",
    "\n",
    "<a id=\"trans1\"></a>\n",
    "### 3.1 Update numeric values\n",
    "Run the `map()` function with the `lambda` keyword to replace each element, X, in your first RDD (the one that has numeric values) with X+1.<br><br>\n",
    "type or copy and paste:<br>\n",
    "x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at all the elements of the new RDD (using collect):<br><br>\n",
    "type or copy and paste:<br>\n",
    "x_nbr_rdd_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with the `collect` method! It returns __all__ elements of the RDD to the driver. Returning a large data set might be not be very useful and may use up all the memory on the driver!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans2\"></a>\n",
    "### 3.2 Split and count text strings\n",
    "\n",
    "Create an RDD with a three text strings and show all elements:<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Words = [\"Hello Human\", \"I am Apache Spark\", \"and I love running analysis on data.\"]\n",
    "words_rd = sc.parallelize(Words)\n",
    "words_rd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD above has three entries, each one being a string: \"Hello Human\", \"I am Apache Spark\", \"and I love running analysis on data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Exercise: Can you write (and execute) in the cell below an action to show the number of entries in the RDD words_rd above ? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now perform a transformation on words_rd. Each string entry will be split (tokenized) into the individual words composing it. This can be achieved using the Python split function. We will be spliting each string on the white space character, but any other character or even a regular expression can be used for splitting strings.\n",
    "The resulting RDD will still have three entries, but instead of being an RDD of strings, it will be an RDD of arrays. Each array will contain the individual tokens composing the original string.<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_rd2 = words_rd.map(lambda line: line.split(\" \"))\n",
    "words_rd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the first element of the resulting RDD. <br><br>\n",
    "type or copy and paste:<br>\n",
    "words_rd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Exercise: Can you display the second element of the first entry of the RDD words_rd2 ? A python array uses square brackets [] and indexing starts at 0.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of elements in this RDD with the `count()` method:<br><br>\n",
    "type or copy and paste:<br>\n",
    "words_rd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now observe the difference between the `map` and `flatMap()` transformations. Notice that flatMap is spelled with an uppercase M !!:<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_rd3 = words_rd.flatMap(lambda line: line.split(\" \"))\n",
    "words_rd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the strings from the original RDD were still split using the white space character, but this time, each array was flattened into its individual components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of entries in this new RDD.<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_rd3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Exercise: Can you think of a scenario where flatMap would be useful ? Do not worry if it is not obvious yet !!, you will be putting it to practice in the next section</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans3\"></a>\n",
    "### 3.3 Count words with a pair RDD\n",
    "A common way to count the number of instances of words in an RDD is to use flatMap first to obtain the individual words. Each word is then augmented with the number 1 to make a tuple. An RDD of tuples is also known as a pair RDD. Because the values in all tuples are 1, when you add the values for a particular word, you get the number of instances of that word. This will be better explained with the example below.\n",
    "\n",
    "Create an RDD:<br><br>\n",
    "Execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]\n",
    "z_str_rdd = sc.parallelize(z)\n",
    "z_str_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the elements into individual words with the `flatmap()` transformation. Notice that since tokens are separated with a comma, this is the separator we use instead of white space.<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))\n",
    "z_str_rdd_split_flatmap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the elements into key-value pairs. We will be augmenting each word with the value 1, and forming a tuple of the format (key,value) where key is a word and value is \"1\":<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))\n",
    "countWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the RDD above, each word such as \"First\", \"Line\" is referred to as the KEY value in the tuple. The number 1 is the VALUE. They both form a (KEY, VALUE) pair, i.e a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will subsequently perform a reduction of the RDD above. For each KEY (word), we will be adding up all the VALUEs. The result of that addition will correspond to the number of time that word was encountered in the RDD.<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "countWords2 = countWords.reduceByKey(add)\n",
    "countWords2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the word `Line` has a count of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Exercise: Can you rewrite below, the cell of code above, replacing the imported function \"add\" with a lambda function defining addition, i.e: lambda a,b: a+b</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filter\"></a>\n",
    "## 4. Filter data\n",
    "\n",
    "The filter command creates a new RDD from another RDD based on a filter criteria.\n",
    "The filter syntax is: \n",
    "\n",
    "`.filter(lambda line: \"Filter Criteria Value\" in line)`\n",
    "\n",
    "Hint: Use a simple python `print` command to add a string to your Spark results and to run multiple actions in a single cell.\n",
    "\n",
    "Find the number of instances of the word `Line` in the `z_str_rdd_split_flatmap` RDD:<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_rd3 = z_str_rdd_split_flatmap.filter(lambda line: \"Second\" in line) \n",
    "\n",
    "print \"The count of words \" + str(words_rd3.first())\n",
    "print \"Is: \" + str(words_rd3.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile\"></a>\n",
    "## 5. Analyze text data from a file\n",
    "In this section, you will download a file from a URL, create an RDD from it, and analyze the text content.\n",
    "\n",
    "<a id=\"wordfile1\"></a>\n",
    "### 5.1 Get the file from a URL\n",
    "\n",
    "You can run shell commands by prefacing them with an exclamation point (!).\n",
    "\n",
    "Remove any files with the same name as the file that you're going to download and then load a file named `README.md` from a URL into the filesystem for Spark:<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm README.md* -f\n",
    "!wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile2\"></a>\n",
    "### 5.2 Create an RDD from the file\n",
    "Use the `textFile` method to create an RDD named `textfile_rdd` based on the `README.md` file. The RDD will contain one element for each line in the `README.md` file.\n",
    "Also, count the number of lines in the RDD, which is the same as the number of lines in the text file.<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textfile_rdd = sc.textFile(\"README.md\")\n",
    "textfile_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile3\"></a>\n",
    "### 5.3 Filter for a word \n",
    "Filter the RDD to keep only the elements that contain the word \"Spark\" with the `filter` transformation:<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\n",
    "Spark_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of elements in this filtered RDD and present the result as a concatenated string:<br><br>\n",
    "Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The file README.md has \" + str(Spark_lines.count()) + \\\n",
    "\" of \" + str(textfile_rdd.count()) + \\\n",
    "\" Lines with word Spark in it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile4\"></a>\n",
    "### 5.4 Count the instances of a string at the beginning of words\n",
    "Display all the tokens in the readme file that start with the string \"Spark\" and count and display the number of times each one of them appears in the original text.\n",
    "For example, if the token \"Sparkly\" appears 10 times in the text file, then your output should display \"Sparkly, 10\"\n",
    "\n",
    "<span style=\"color:blue\">Here's what you need to do (the code is left as an exercise): </span>\n",
    "\n",
    "1. Run a `flatMap` transformation on the Spark_lines RDD and split on white spaces.\n",
    "2. Create an RDD with key-value pairs where the first element of the tuple is the word and the second element is the number 1.\n",
    "3. Run a `reduceByKey` method with the `add` function to count the number of instances of each word.<br>\n",
    "4. Filter the resulting RDD to keep only the elements that start with the word \"Spark\". In Python, the syntax to determine whether a string starts with a token is: `string.startswith(\"token\")` \n",
    "5. Display the resulting list of elements that start with \"Spark\" and their counts.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile5\"></a>\n",
    "### 5.5 Count instances of a string within words\n",
    "Now instead of displaying the elements and counts of tokens that start with \"Spark\", display all tokens and coounts where the substring \"Spark\" appears anywhere in the word. Your result should be a superset of the previous result.\n",
    "\n",
    "The Python syntax to determine whether a string contains a particular token is: `\"token\" in string`<br><br>\n",
    "<span style=\"color:blue\"> The code for this cell is left as an exercise</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"numfile\"></a>\n",
    "## 6. Analyze numeric data from a file\n",
    "You will now analyze a simple file that contains instructor names and scores. The file has the following format: Instructor Name,Score1,Score2,Score3,Score4. \n",
    "Here is an example line from the text file: \"Carlo,5,3,3,4\"\n",
    "\n",
    "Add all scores and report on the average result per instructor:\n",
    "\n",
    "<span style=\"color:blue\">Here are the steps that need to be implemented:</span>\n",
    "1. The name of the file is \"Scores.txt\". Delete it from the local filesystem if it exists.\n",
    "2. Download the file from the provided location (see below).\n",
    "3. Load the text file into an RDD of instructor names and instructor scores.\n",
    "4. Run a transformation to create an RDD with the instructor names and the sum of the 4 scores per instructor.\n",
    "5. Run a second transformation to compute the average score for each instructor.\n",
    "6. Display the first 5 results.<br>\n",
    "\n",
    "The Data File has the following format: Instructor Name,Score1,Score2,Score3,Score4<br>\n",
    "Here is an example line from the text file: \"Carlo,5,3,3,4\"<br>\n",
    "Data File Location: https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}